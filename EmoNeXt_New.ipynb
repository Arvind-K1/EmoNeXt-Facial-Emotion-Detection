{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNEthjWkpXgTBSyQIcRQhdq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arvind-K1/EmoNeXt-Facial-Emotion-Detection/blob/main/EmoNeXt_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xeWPAw3phUl",
        "outputId": "1ffca82a-a402-4bbb-da2a-14c78c57e5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/root/.cache/kagglehub/datasets/msambare/fer2013/versions/1/train\" #passing the path with training images\n",
        "test_dir = \"/root/.cache/kagglehub/datasets/msambare/fer2013/versions/1/test\""
      ],
      "metadata": {
        "id": "-v1A-LXOpmf8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n"
      ],
      "metadata": {
        "id": "t1WHerC3pi0_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \"\"\"\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError\n",
        "        self.normalized_shape = (normalized_shape,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) if layer_scale_init_value > 0 else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        if self.gamma is not None:\n",
        "            x = self.gamma * x\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = input + self.drop_path(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HtMwAJkvpo-g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=7),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Calculate the input size dynamically\n",
        "        # Assuming an input image size of 224x224\n",
        "        dummy_input = torch.randn(1, 1, 224, 224)  # (batch_size, channels, height, width)\n",
        "        dummy_output = self.localization(dummy_input)\n",
        "        fc_input_size = dummy_output.shape[1] * dummy_output.shape[2] * dummy_output.shape[3]\n",
        "\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(fc_input_size, 32), # Change input size here\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        xs = self.localization(x)\n",
        "        # Reshape dynamically\n",
        "        xs = xs.view(xs.size(0), -1)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "        grid = nn.functional.affine_grid(theta, x.size())\n",
        "        x = nn.functional.grid_sample(x, grid)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yJneYugaqH12"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Squeeze-and-Excitation Block\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "metadata": {
        "id": "5N4-rN_CqNFr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# EmoNeXt Architecture\n",
        "class EmoNeXt(nn.Module):\n",
        "    def __init__(self, in_chans=1, num_classes=7,\n",
        "                 depths=[3, 3, 9, 3], dims=[64, 128, 256, 512],\n",
        "                 drop_path_rate=0.1, layer_scale_init_value=1e-6, head_init_scale=1.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stn = SpatialTransformer()\n",
        "        self.downsample_layers = nn.ModuleList()\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList()\n",
        "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j],\n",
        "                        layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])],\n",
        "                SEBlock(dims[i])\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        self.head.weight.data.mul_(head_init_scale)\n",
        "        self.head.bias.data.mul_(head_init_scale)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:  # Check if bias exists before initializing\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.stn(x)\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(x.mean([-2, -1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "QIB_nle9qndH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training and Evaluation Pipeline\n",
        "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
        "    best_test_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100. * train_correct / train_total\n",
        "\n",
        "        # Evaluation Phase\n",
        "        model.eval()\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Evaluation]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = outputs.max(1)\n",
        "                test_total += labels.size(0)\n",
        "                test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        test_accuracy = 100. * test_correct / test_total\n",
        "        if test_accuracy > best_test_accuracy:\n",
        "            best_test_accuracy = test_accuracy\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(f\"Best Test Accuracy: {best_test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "pt6-1AZoqsei"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "R71P6Aekqup5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((224, 224)),  # Resize for pretrained weights\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n"
      ],
      "metadata": {
        "id": "OZ5TPJoRqwr3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Datasets and Dataloaders\n",
        "train_dataset = ImageFolder(train_dir, transform=transform)\n",
        "test_dataset = ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "2PocA2Miqywg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model, Loss, Optimizer, and Scheduler\n",
        "model = EmoNeXt(in_chans=1, num_classes=7)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)"
      ],
      "metadata": {
        "id": "IN2yWUd7q1Vw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train and Evaluate\n",
        "train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4QbEPKRpqEs",
        "outputId": "bd41cb37-1ce0-40bf-dc4f-168a31b106c4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Training]: 100%|██████████| 449/449 [04:14<00:00,  1.77it/s]\n",
            "Epoch 1/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10: Train Loss: 1.8251, Train Acc: 23.99%, Test Acc: 24.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 [Training]: 100%|██████████| 449/449 [04:12<00:00,  1.78it/s]\n",
            "Epoch 2/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10: Train Loss: 1.8054, Train Acc: 25.09%, Test Acc: 24.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 [Training]: 100%|██████████| 449/449 [04:13<00:00,  1.77it/s]\n",
            "Epoch 3/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10: Train Loss: 1.7888, Train Acc: 25.25%, Test Acc: 26.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 [Training]: 100%|██████████| 449/449 [04:13<00:00,  1.77it/s]\n",
            "Epoch 4/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10: Train Loss: 1.7506, Train Acc: 27.48%, Test Acc: 28.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 [Training]: 100%|██████████| 449/449 [04:13<00:00,  1.77it/s]\n",
            "Epoch 5/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10: Train Loss: 1.7220, Train Acc: 29.16%, Test Acc: 29.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Training]: 100%|██████████| 449/449 [04:13<00:00,  1.77it/s]\n",
            "Epoch 6/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10: Train Loss: 1.6975, Train Acc: 30.96%, Test Acc: 32.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 [Training]: 100%|██████████| 449/449 [04:13<00:00,  1.77it/s]\n",
            "Epoch 7/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10: Train Loss: 1.6693, Train Acc: 33.13%, Test Acc: 33.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 [Training]: 100%|██████████| 449/449 [04:13<00:00,  1.77it/s]\n",
            "Epoch 8/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10: Train Loss: 1.6500, Train Acc: 33.96%, Test Acc: 34.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 [Training]: 100%|██████████| 449/449 [04:13<00:00,  1.77it/s]\n",
            "Epoch 9/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10: Train Loss: 1.6376, Train Acc: 34.63%, Test Acc: 35.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 [Training]: 100%|██████████| 449/449 [04:13<00:00,  1.77it/s]\n",
            "Epoch 10/10 [Evaluation]: 100%|██████████| 113/113 [00:23<00:00,  4.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10: Train Loss: 1.6321, Train Acc: 35.34%, Test Acc: 35.32%\n",
            "Best Test Accuracy: 35.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}